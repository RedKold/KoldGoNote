# 序言

本笔记参考书籍：
[[大二第一学期/金融实践/机器学习实战：基于Scikit-Learn、Keras和TensorFlow：原书第2版 (Aurélien Géron) (Z-Library).pdf|机器学习实战：基于Scikit-Learn、Keras和TensorFlow：原书第2版 (Aurélien Géron) (Z-Library)]]
机器学习（周志华）

# 分类

> 回归（Regression）和分类（Classification）是机器学习中两个最重要的概念

## Confusion Matrix

Confusion Matrix 是一个有趣的概念。
对于二元分类而言
(0,0):FN (0,1):FP
(1,0):TN (1,1):TP
TN: (True Negtive)
真阴性，实际结果为0，是负类
FN: (False Negtive)
假阴性，实际结果为1，是正类
TP: (True Positive)
真阳性，实际结果为1，是正类
FP: (False Positive)
假阳性，实际结果为0，是负类
True OR False 是衡量预测的正确与否
Negtive OR Positive 是实际取值问题
Confusion Matrix提供了许多评价模型的信息。
![[Pasted image 20240816161617.png#pic_center|600]]

$$
{Precision}=\frac{TP}{TP+FP} （精度）
$$

$$
{Recall}=\frac{TP}{TP+FN} （召回率）
$$

### 精度和召回率的进一步考虑-PR曲线

你面临精度/召回率权衡

由于SGDClassifier的分类原理：它会基于决策函数计算出一个分值，如果该值大于阈值，就会把该实例判定为正类否则判定为负类

如图所示：

![[Pasted image 20240816100849.png]]

如果阈值分数较小（靠左）--> 那么就有精度下降，召回率增加
如果阈值分数较大（靠右）--> 那么就有精度提高，召回率下降

面临不同的实际问题可能你会有不同的抉择:

下面展示一个用matplotlib绘画的Precision和Recall的相关图像
绘画代码：（所用数据库MNIST, 识别手写数字5）

```python
from sklearn.metrics import precision_recall_curve

precisions,recalls,thresholds=precision_recall_curve(y_train_5,y_scores)

  

def plot_precision_recall_vs_threshold(precisions,recalls,thresholds):

    line1,_=plt.plot(thresholds,precisions[:-1],'b--',label="Precision")

    line2,_=plt.plot(thresholds,recalls[:-1],'g-',label='recall')

    plt.legend(handles=[line1,line2],labels=['Precision','Recall'])

  

plot_precision_recall_vs_threshold(precisions,recalls,thresholds)

plt.show()
```

![[Pasted image 20240816105124.png]]

这个图可以给你提供权衡的决策依据

## ROC曲线

经常和二元分类器一起使用
受试者工作特征曲线(ROC)：receiver operating characteristic curve

类似于 精度/召回率曲线
绘制的是 真正类率（召回率的别名，也就是在正类中，真正的正类比率） 和 假正类率(FPR)

## 多类分类器

有一些算法（如随机森林分类器 或 朴素贝叶斯分类器）可以直接处理多个类。
也有一些严格的二元分类器（如支持向量机分类器或线性分类器）。但是，有多种策略可以让你用几个二元分类器实现多类分类的目的。

python库 **sklearn.multiclass** 可以解决多类别 (multi-class) 的多标签 (multi-label) 的分类问题。

### 一对剩余策略（OvR, one-versus-all）

例如识别0-9的10个数字，可以**训练10个二元分类器**。识别一张图片的时候，就调用10个二元分类器，选取分数最高的，分到对应的类。

不难知道训练$\mathrm{N}$个类别，需要 $N$个分类器。
对于MNIST这个类别数量是：**10个**

```python
#强制使用ovr模型（正常状况,SVC()会自己挑选策略）
from sklearn.multiclass import OneVsRestClassifier
ovr_clf = OneVsRestClassifier(SVC())#只需要将分类器传给ovr即可
ovr_clf.fit(X_train, y_train)
ovr_clf.predict([some_digit])
len(ovr_clf.estimators_)
```

output

```python
10
```

### 一对一策略（OvO, one-versus-one）

为每一对数字组合训练一个二元分类器：一个区分0, 1 一个区分0, 2 ······

不难知道训练$\mathrm{N}$个类别，需要 $C_{N}^{2}=\frac{N\times{(N-1)}}{2}$个分类器。
对于MNIST这个类别数量是：**45个**

优点：每个分类器只需要部分训练集训练。

对于多数情况，OvR是更好的选择。除了在数据规模扩大很多的情况下：因为OvO在较小训练集上分别训练多个分类器，这种分而治之，要比大型数据集训练少数分类器快得多。

如果想要强制Scikit-Learn使用一对一或者一对剩余策略，可以使用OneVsOneClassifier或OneVsRestClassifier类。只需要创建一个实例，然后将分类器传给其构造函数（它甚至不必是二元分类器）。例如，下面这段代码使用OvR策略，基于SVC创建了一个多类分类器：

## 误差分析

在做完预测之后，需要做误差分析。这里用sgd_clf^[SGD (Stochastic Gradient Descent), 随机梯度下降分类]来做例子，用matplotlib画出confusion matrix帮助分析

```python
y_train_pred=cross_val_predict(sgd_clf,X_train_scaled,y_train,cv=3)

#create confusion matrix

conf_mx=confusion_matrix(y_train,y_train_pred)

plt.matshow(conf_mx, cmap='gray')

plt.show()
```

![[SGD分类器-confusion matrix可视化.png|500x500]]

现在，我们对confusion matrix正则化处理，获得错误率的更合理化的表示，来看看这个模型的分类情况

记住，<font color=red>**每行代表实际true类，而每列表示预测pred类。**</font>
![[Figure_1_normalized_confusion_matrix.png|500x500]]
这里就可以看到，我们的第3列表现不好，列代表预测类，说明有很多数字被错误地分类到了'3'
而第3行表现还不错，说明多数的'3'被正确分类到了'3'

row 5, column 3表现尤为差劲，说明存在很多实例，真实值是5，预测值错误的为3.


# 决策树

## Def
决策树是一类常见的机器学习方法. 以二分类任务为例，我们将分类任务看做“当前样本属于正类吗？”这个问题的“决策”过程。顾名思义，决策树使用树结构来进行决策，这是人类处理决策问题的很自然的处理机制。

&emsp; 决策树组成：
	根结点：包含样本全集
	若干内部结点：属性测试
	若干叶节点：决策结果
决策树学习：产生一棵泛化能力强的决策树（目的）

![[DecisionTree.png|300x]]
显然是一个递归过程。
&emsp; 递归返回的三种情形：
1. 当前结点包含的样本都属于同一类别 无需划分
2. 当前属性集为空，或是所有样本在所有属性上取值相同，无法划分
3. 当前结点包含的样本集合为空，不能划分

## 划分选择

### 信息增益：
“信息熵”是度量样本集合纯度最重要的指标。
假定当前样本集合D中第$k$类样本所占的比例为$p_{k}$($k=1,2,\dots,|Y|$)，则$D$的信息熵定义为

$$
\mathrm{Ent(D)=-\sum_{k=1}^{|Y|}{p_{k} \log_{2}p_{k}}}
$$
假定李三属性$a$有*V*个可能的取值$\{ a^1,a^2,\dots,a^V \}$，若使用$a$对样本进行划分，会产生V个分支结点。
其中第$v$个分支结点包含了$D$中所有在属性$a$上取值为$a^v$的样本，记作$D^v$

可计算$D^v$的信息熵，再考虑到不同的分支结点包含的样本数不同，赋予分支结点权重$\dfrac{|D^v|}{|D|}$,即样本数越多的分支结点的影响越大，就可计算出用属性$a$对样本集$D$进行划分所获得的**信息增益**(information gain)
$$
\mathrm{Gain(D,a)=Ent(D)-\sum_{v=1}^{V}{\frac{|D^v|}{|D|}Ent(D^v)}}
$$
$Ent(D)$表达的是体系的总熵，减去这个结点的信息熵$\sum_{v=1}^{V}{\frac{|D^v|}{|D|}Ent(D^v)}$得到了就是信息熵”下降“的指标，衡量了**信息量的增加**，或者说**纯度提升**

ID3(Iterative Dichotomiser(迭代二分器))决策树学习算法就是以**信息增益**为原则来选择划分属性。

### 剪枝处理
剪枝（pruning）是决策树学习算法对付“过拟合”的主要手段。
即主动去掉一些分支，来避免把训练集自身的一些特点当做所有数据都具备的一般性质而导致过拟合。

# 神经网络
## 神经元模型

&emsp;神经网络(neural networks)方面的研究很早就已出现。

### Def
>神经网络是由具有适应性的简单单元组成的广泛并行互连的网络，它的组织能够模拟生物神经系统对真实世界物体所作出的交互反应 [Kohonen,1998]

在机器学习中谈论神经网络，说的是“神经网络学习”，或说是神经网络和机器学习的交叉部分

神经元模型：即上述定义中的简单单元。

### M-P 神经元模型
神经元接收到来自n个其他神经元传递过来的输入信号，这些输入信号通过带权重的连接(connection)进行传递，神经元接收到的总输入值将与神经元的阈值进行比较，然后通过**激活函数**(activation function)处理以产生神经元的输出。

理想的激活函数是**阶跃函数**^[将输入值映射为输出值0或1]，缺点是不太光滑。
$$
sgn(x)=
\begin{cases}
1,&x\geq0\\ \\
0,&x<0\\
\end{cases}
$$
实际常用**Sigmoid函数**作为激活函数。常见的Sigmoid函数把可能在较大范围内变化的输入值挤压到(0,1)输出范围内，有时也称为挤压函数(squashing function)  ^fdc257
$$
\mathrm{sigmoid(x)=\frac{1}{1+e^{-x}}}
$$

^ff8244

用Python画出Sigmoid函数
```python
import torch
def sigmoid(x):
    y=torch.sigmoid(x)
    return y

import numpy as np
import matplotlib.pyplot as plt

x=torch.arange(-10,10,0.1)
plt.plot(x,sigmoid(x))
plt.show()

```
![[Sigmoid.png|300x]]
### 感知机与多层神经网络

**感知机(Perceptron)** 由两层神经元组成，输入层接受外界输入信号后传递给输出层，输出层是M-P神经元，亦称“阈值逻辑单元”(threshold logic unit)

感知机能容易地实现与或非运算。注意到$y=f\left( \sum w_{i}x_{i}-\theta \right)$, 假定$f$作为[[#^fdc257|阶跃函数]]，那么
- **与**$x_{1}\land x_{2}$：令$w_{1}=1,w_{2}=1,\theta =2$ 则$y=f(1\cdot x_{1}+1\cdot x_{2}-2$)，当且仅当$x_{1}=x_{2}=1,y=1$
- **或**$x_{1}\lor x_{2}$：令$w_{1}=1,w_{2}=1,\theta =0.5$ 则$y=f(1\cdot x_{1}+1\cdot x_{2}-0.5$)，当$x_{1}=1 或 x_{2}=1,y=1$
- **非**$\lnot{x_{1}}$：令$w_{1}=-0.6,w_{2}=0,\theta=-0.5,y=f(-0.6\cdot x_{1}+0\cdot x_{2}+0.5)$，当$x_{1}=1$时，$y=0$;当$x_{1}=0$时，$y=1$
- 值得注意的是：这里的参数$w_{1},w_{2},\theta$参数均不唯一.有多种取值.只要满足阶跃条件就可以了

这里我们是**聪明的**发现了参数. 更一般的, 实际应用中, 我们可以学习权重来得到参数值. 
可以将阈值 $\theta$看作一个输入固定位-0.1的哑结点(dummy node), 这样就把阈值和权重的学习统一为权重的学习(互相适应的)

感知机只有输出层神经元进行激活函数处理, 即只拥有一层功能性神经元, 学习能力非常有限. 上述与, 或, 非都是**线性可分**问题(linearly separable). 若两类模式是线性可分的,即存在一个线性超平面能将他们分开,则感知机的学习过程一定会收敛(converge)而求得适当的权向量$\mathbf{w}=(w_{1};w_{2};\cdots;w_{n+1})$;
否则感知机学习过程会振荡(fluctuation)

例如异或,就是**非线性可分的**

解决非线性可分问题, 考虑使用**多层功能神经元**. 

![[异或问题神经网络结构图.png|300x]]
	In picture, the layer between input layer and ouput layer, is called **hidden layer**
隐含层和输出层都有激活函数, 是功能性神经元(functional neuron)

更一般的, 常见的神经网络是层级结构, 每层神经元和下一层全互连, 神经元之间不存在同层连接, 也不存在跨层连接. 这样的神经网络被称为**"多层前馈神经网络"**^[向前反馈的意思吗?] (multi-layer feedforward neural networks)

只需包含hidden layer, 即可称为多层网络. 
神经网络的训练过程就是通过训练数据来调整神经元之间的**连接权**(connection weight)以及每个功能神经元的阈值. 阈值和连接权重矩阵就是神经网络**"学"**到的东西
### 误差逆传播算法

误差逆传播算法(error BackPropagation, **BP**)是迄今为止最成功的神经网络学习算法.

现在来研究一下误差逆传播算法

给定训练集$D=\{(x_{1},y_{1},(x_{2},y_{2}),\dots,(x_{m},y_{m})\},x_{i}\in \mathbb R^d,y_{i} \in\mathbb R^l)$
输入实例由$d$个属性描述, 输出$l$维实值向量.




现在以一个拥有$d$个输入神经元，$l$个输出神经元，$q$个隐层神经元的多层前馈神经网络结构，其中：

- $输出层第j个神经元的阈值用\theta_{j}表示$
- $隐藏层第h个神经元的阈值用\gamma_{h}表示$
- $输入层第i个神经元与隐层第h个神经元之间的连接权用v_{ih}表示$
- $隐藏层第h个神经元与输出层第j个神经元之间的连接权用w_{hj}表示$
- $记隐层第h个神经元接收到的输入为\alpha_{i}=\sum_{i=1}^d{v_{ih}x_{i}}$
- $输出层第j个神经元接收到的输入为\beta_{j}=\sum_{h=1}^q{w_{hj}b_{h}}其中b_{h}是隐层第h个神经元的输出.$
- **假设隐层和输出层都使用sigmoid函数**

$训练例(x_{k},y_{k}),假定神经网络的输出为\hat{y_{k}}=(\hat{y}^k_{1},\hat{y}^k_{2},\cdots,\hat{y}^k_{l})，即$
$$
\hat{y}_{j}^k =f(\beta_{j}-\theta_{j})\tag{0}
$$
$那么均方误差为：$
$$
E_{k}=\frac{1}{2}\sum_{j=1}^l(\hat{y}_{j}^k-{y}_{j}^k)^2 \tag{1}
$$
需要确定的参数：共$(d+l+1)q+l$个
	$输入层到隐层: d\times q$
	$隐层到输出层:q\times l$
	$q个隐藏神经元的阈值$
	$l个输出层神经元的阈值$
BP迭代 求出这些参数。

以$w_{hj}$为例子展示推导过程。
**BP**算法基于**梯度下降(gradient descent)策略**,以目标的负梯度方向对参数进行调整. 

$$
\Delta w_{hj}=-\eta\frac{\partial E_k}{\partial w_{hj}}\tag{2}
$$
根据**链式法则**，可以知道：
（最终的误差可以写作一个复合函数，实际上是多个神经网络层之间的复合运算，这个偏导可以用链式法则来解决）
$$
\frac{\partial E_{k}}{\partial w_{hj}}=\frac{\partial E_{k}}{\partial \hat{y}^k_{j}}\cdot \frac{{\partial \hat{y}_{j}^k}}{\partial \beta_{j}}\cdot\frac{\partial \beta_{j}}{\partial w_{hj}}\tag{3}
$$

[[#^ff8244|Sigmoid函数]]有一个很好的性质：
$$
f'(x)=f(x)(1-f(x))\tag{4}
$$
根据式子(0), (1):
$$\begin{aligned}
g_{i}=-{\frac{\partial E_{k}}{\partial \hat{y}^k_{j}}}\cdot \frac{{\partial \hat{y}_{j}^k}}{\partial \beta_{j}} \\ 
=\hat{y}_{j}^k (1-\hat{y}_{j}^k)(y_{j}^k-\hat{y}_{j}^k)
\end{aligned}
$$

于是联立$(1),(2),(3),(4)$可以求解出BP算法中关于$w_{hj}$的更新公式：
$$
\Delta w_{hj}=\eta g_{j}b_{h}\tag{5}
$$
类似得到：
$$\begin{matrix}
\Delta \theta_{j}=-\eta g_{j} \tag{6 }\\ 
\Delta v_{ih}=\eta e_{h}x_{i} \\
\Delta \gamma_{h}=-\eta e_{h} \\
\end{matrix}
$$

其中
$$\begin{aligned}
e_{h}=-\frac{\partial Ek}{\partial b_{h}}\cdot \frac{\partial b_{h}}{\partial \alpha_{h}}\\
=\sum_{j=1}^lw_{hj}g_{j}f'(\alpha_{h}-\gamma_{h})\\
=b_{h}(1-b_{h})\sum_{j=1}^lw_{hj}g_{j}
\end{aligned}
$$
学习率$\eta \in (0,1)$决定着算法中每一轮迭代中的步长. 太大容易振荡, 太小则收敛速度过慢. 
![[误差逆传播算法.png|500]]

### 深度学习

典型的深度学习模型就是很深层的神经网络. 显然, 神经网络模型, 提高容量的一个简单办法是增加隐层的数目. 隐层多了, 相应的神经元连接权, 阈值等参数就会变多. 

#### 实践训练
下面尝试用Pytorch训练一个神经网络，在MNIST数据集上运行

#### 训练代码
```python
import torch

import torch.nn as nn

import torch.nn.functional as F

import numpy as np

from matplotlib import pyplot as plt

from sklearn.datasets import fetch_openml

  
  

datasets=fetch_openml("mnist_784",version=1,as_frame=False)

  

print(datasets.keys())

  

#img

X=datasets['data']

#labels

y=datasets['target']

  

# 将数据转换为浮点数

X = X.astype(np.float32)  # 转换为 float32 类型以便与 PyTorch 兼容

  

# 将标签转换为整数

y = y.astype(np.int64)  # 转换为 int64 类型以便与 PyTorch 兼容

#例举一些图显示出来

  

'''for i in range(12):

    plt.subplot(3,4,i+1)

    plt.tight_layout()

    plt.imshow(X[i].reshape(28,28),cmap='gray')

    plt.title("Label:{}".format(y[i]))

    plt.xticks([])

    plt.yticks([])

    plt.show()'''

  

class Net(nn.Module):

    def __init__(self):

        super(Net,self).__init__()

        #三个线性层

        self.fc1=nn.Linear(28*28,64)

        self.fc2=nn.Linear(64,64)

        self.fc3=nn.Linear(64,64)

        self.fc4=nn.Linear(64,10)

    # parameter x：传入图片

    # 定义向前传播代码

    def forward(self,x):

        x=F.relu(self.fc1(x))

        x=F.relu(self.fc2(x))

        x=F.relu(self.fc3(x))

        x=F.log_softmax(self.fc4(x),dim=-1)

        return x

  

#划分训练集和测试集

X_train=torch.from_numpy(X[:3000])

y_train=torch.from_numpy(y[:3000])

X_test=torch.from_numpy(X[3000:])

y_test=torch.from_numpy(y[3000:])

  

#set the model

net=Net()

  

#create loss list

losses=[]

  

def Training(X_train,y_train,epochs):

    #确定损失分析器和优化器

    criterion=torch.nn.CrossEntropyLoss()

    optimizer=torch.optim.SGD(net.parameters(),lr=0.001)

    for epoch in range(epochs):

        for i in range(3000):

            running_losses=0.0

            net.zero_grad()

            #print("X_train shape",X_train[i].shape)

            #print("X_train.type",X_train[i].__type__)

            X_train_i=X_train[i].view(-1,28*28)

            output=net(X_train_i)

            loss=criterion(output,y_train[i].unsqueeze(0))

            running_losses+=(loss.item())#.item()返回标量

            loss.backward()

            optimizer.step()

            if i%30==1:

                losses.append(running_losses/300)

                running_losses=0.0

  

#super Parameters

import random

epochs=2

  

if __name__=='__main__':

    Training(X_train,y_train,epochs)

    #随机抽取十个下标来检测结果

    Ranlist=torch.randperm(X_test.size(1))

    for i in Ranlist[:10]:

        pred=net(X_test[i])

        print("Pred:",torch.argmax(pred,dim=0)," True ans is",y_test[i])

  

    #Draw the Loss curve

    plt.title("Error curve")

    x=range(6000)

    plt.plot(range(len(losses)),losses)

    plt.show()
```


#### 评估
这个模型，展示了用PyTorch搭建神经网络框架的一般流程。
对于PyTorch提供的Linear神经网络的具体实现，**精华部分**可以参考前面写到的误差逆传播的算法

# 支持向量机

## Def
**支持向量机**(Support Vector Machines, SVM)是一种**二分类模型**，它的目的是寻找一个超平面来对样本进行分割，分割的原则是间隔最大化，最终转化为一个凸二次规划问题来求解

下面我们一步一步领会一下对支持向量机的定义里的专业词汇都是什么意思
### 支持向量和间隔
给定训练样本集合$D=\{ (\pmb{x}_{1},y_{1}),(\pmb{x}_{2},y_{2}),\cdots ,(\pmb{x}_{m,},y_{m}),y_{i}\in \{ -1,+1 \}\}$
分类学习：基于训练集D在样本空间中找到一个划分超平面，将不同类别的样本分开。

该如何去寻找呢？

直观来看——找位于两类训练样本“正中间”的划分超平面比较好。

在样本空间中，划分超平面可通过如下线性方程描述：
$$
\pmb{w}^T\pmb{x}+b=0, \tag{1}
$$
where $\pmb{w}=(w_{1};w_{2};\dots;w_{d})$为法向量，决定超平面**方向**；
$b$为位移项，决定了超平面与原点的**距离**。

显然划分超平面，可以由法向量$\pmb{w}$和位移项$b$确定
将超平面记为$(\pmb{w},b)$，样本空间中任意点$\pmb{x}$到超平面$(\pmb{w},b)$的距离可写为
$$
r=\frac{{|w^Tx+b|}}{||w||} \tag{2}
$$
超平面$(w,b)$如果被正确的分裂，那么对于$(x_{i},y_{i})\in D$，若$y_{i}=+1$ 则有$w^Tx_{i}+b>0$; 若$y_{i}=-1$，则有$w^Tx_{i}+b<0$

用符号语言进一步表示，得到下面的式子：
$$
\begin{cases}
w^Tx_{i}+b\geq+1, y_{i}=+1 \\
w^Tx_{i}+b\leq-1,y_{i}=-1 \tag{3}
\end{cases}
$$
使得式（3）中**等号**成立的训练样本点，也就是距离超平面最近的点，叫做“**支持向量**”(support vector)
两个异类支持向量到超平面的距离之和为
$$
\gamma=\frac{2}{||\pmb{w}||}
$$
它被称为“间隔”(margin)

欲找到具有“最大间隔”(maximum margin)的划分超平面，也就是要找到能满足式(6.3)中约束的参数$w$ 和$b$，使得$\gamma$最大，即
$$
\underset{w,b}{max} \mathbf{}{\frac{2}{||w||}}, s.t. y_{i}(w^Tx_{i}+b)\geq 1, i=1,2,\cdots,m \tag{4}
$$
显然为了最大化间隔，只需要最大化$\mathbf{||w||^{-1}}$，这等价于最小化$||w||^2$ 
$$\begin{aligned}
&&&\underset{w,b}{min} \frac{1}{2}||w||^2\\
&&&s.t. y_{i}(w^Tx_{i}+b)\geq 1,i=1,2,\cdots,m
\end{aligned} \tag{5}
$$^[s.t. : subject to: 服从于]
### 对偶问题
我们希望求解式(5)来获得间隔划分超平面模型
$$
f(\pmb{x})=\pmb{w}^T\pmb{x}+b
\tag{6}$$
其中$w$和$b$是模型参数。

式(5)本身是一个**凸二次规划**(convex quadratic programming)问题（因为$f(w)=\frac{1}{2}||w||^2是凸函数$），能直接用现成的优化计算包求解，但我们可以有更高效的办法.